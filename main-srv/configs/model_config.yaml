# =============================================
# Конфигурация подключения к LLM-серверу (llama-server)
# Версия: 1.0.0
# Описание: Параметры для взаимодействия с Qwen3-8B через OpenAI-compatible API
# =============================================

llm_server:
  # Адрес сервера, на котором запущен llama-server (scripts/model_orchestrator.sh)
  # Меняется без правки кода — для переключения между серверами
  host: "main-srv"
  
  # Порт llama-server (должен совпадать с --port в скрипте запуска)
  # По умолчанию 8081, можно изменить при конфликте портов
  port: 8081
  
  # Протокол подключения: http (локально) или https (продакшен)
  protocol: "http"
  
  # Endpoint API llama-server (стандартный OpenAI-compatible путь)
  # Не менять, если llama-server не кастомизирован
  endpoint: "/v1/chat/completions"
  
  # Таймаут ожидания ответа от модели в секундах
  # Увеличить для сложных запросов с длинной генерацией
  timeout: 120

model:
  # Имя файла модели (должно совпадать с тем, что видит llama-server)
  # Используется в поле "model" при запросе к API
  name: "Qwen3-8B-Q4_K_M.gguf"
  
  # Размер контекста модели (токенов)
  # Должен совпадать с --ctx-size в скрипте запуска llama-server
  # Возвращается в metrics.host_nctx для аналитики в БД
  n_ctx: 8192

retry:
  # Максимальное количество попыток запроса при временных сбоях (сеть, 5xx)
  # Увеличить для нестабильных соединений, уменьшить для быстрых ошибок
  max_attempts: 3
  
  # Базовая задержка между попытками в секундах
  # Используется экспоненциальный рост: 1с → 2с → 4с → ...
  backoff_seconds: 1

queue:
  # Включить FIFO-очередь запросов (защита от перегрузки llama-server)
  # true = запросы ждут в очереди, false = мгновенная ошибка при перегрузке
  enabled: true
  
  # Максимальное количество запросов в очереди
  # Защита от переполнения памяти при массовых параллельных вызовах
  max_size: 10