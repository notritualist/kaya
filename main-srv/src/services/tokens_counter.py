"""
main-srv/src/services/tokens_counter.py

–ú–æ–¥—É–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á—ë—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –º–æ–¥–µ–ª–∏ Qwen3.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É 'tokenizers' (–±—ã—Å—Ç—Ä—ã–π Rust-–±—ç–∫–µ–Ω–¥ –æ—Ç HuggingFace).
–≠—Ç–æ –ª—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ AGI:
- –¢–æ—á–Ω–æ—Å—Ç—å 100% —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –º–æ–¥–µ–ª—å—é Qwen3
- –†–∞–±–æ—Ç–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ñ–ª–∞–π–Ω (–ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª tokenizer.json)
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∑–∞ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã, –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç –º–∏–Ω–∏–º—É–º –ø–∞–º—è—Ç–∏
- –ü—Ä–æ—Å—Ç–æ–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π –∫–æ–¥ –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É: pip install tokenizers
2. –ü–æ–ª–æ–∂–∏—Ç—å —Ñ–∞–π–ª tokenizer.json –≤ –ø–∞–ø–∫—É: models/qwen3-8b-tokenizer/
   (—Ñ–∞–π–ª –º–æ–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å —Å HuggingFace: https://huggingface.co/Qwen/Qwen3-8B)
"""

__version__ = "1.0.0"
__description__ = "–ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è Qwen3 (–æ–¥–∏–Ω –¥–≤–∏–∂–æ–∫: tokenizers)"

import logging
from pathlib import Path
from functools import lru_cache

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
# –ï—Å–ª–∏ –æ–Ω–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ ‚Äî –ø—Ä–æ–≥—Ä–∞–º–º–∞ —Å—Ä–∞–∑—É —Å–æ–æ–±—â–∏—Ç –æ–± —ç—Ç–æ–º –ø–æ–Ω—è—Ç–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
try:
    from tokenizers import Tokenizer
except ImportError:
    # –õ–æ–≥–≥–µ—Ä –º–æ–∂–µ—Ç –µ—â—ë –Ω–µ –±—ã—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è,
    # –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π logging –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ –Ω–∞ —Å—Ç–∞—Ä—Ç–µ
    logging.critical(
        "‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'tokenizers' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!\n"
        "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë –∫–æ–º–∞–Ω–¥–æ–π: pip install tokenizers\n"
        "–ë–µ–∑ –Ω–µ—ë –º–æ–¥—É–ª—å –ø–æ–¥—Å—á—ë—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ –±—É–¥–µ—Ç."
    )
    raise ImportError("–¢—Ä–µ–±—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'tokenizers'. –í—ã–ø–æ–ª–Ω–∏—Ç–µ: pip install tokenizers")

# –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–≥–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è.
# –û–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Ö–≤–∞—Ç–∏—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ main.py (—Ñ–∞–π–ª + –∫–æ–Ω—Å–æ–ª—å, —É—Ä–æ–≤–Ω–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
logger = logging.getLogger(__name__)


# === –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô ===
# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞, –ø–æ–¥–Ω–∏–º–∞—è—Å—å –Ω–∞ 3 —É—Ä–æ–≤–Ω—è –≤–≤–µ—Ä—Ö –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞:
# tokens_counter.py ‚Üí services ‚Üí src ‚Üí main-srv ‚Üí kaya (–∫–æ—Ä–µ–Ω—å)
_CURRENT_FILE = Path(__file__).resolve()
_PROJECT_ROOT = _CURRENT_FILE.parent.parent.parent

# –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º.
# –ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –ª–µ–∂–∞—Ç—å —Ñ–∞–π–ª tokenizer.json, —Å–∫–∞—á–∞–Ω–Ω—ã–π —Å HuggingFace –¥–ª—è –º–æ–¥–µ–ª–∏ Qwen3-8B
_TOKENIZER_DIR = _PROJECT_ROOT / "models" / "qwen3-8b-tokenizer"
_TOKENIZER_FILE = _TOKENIZER_DIR / "tokenizer.json"


# === –ì–õ–û–ë–ê–õ–¨–ù–´–ô –≠–ö–ó–ï–ú–ü–õ–Ø–† –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê (Singleton) ===
# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Ö—Ä–∞–Ω–∏—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, —á—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª —Å –¥–∏—Å–∫–∞ –∫–∞–∂–¥—ã–π —Ä–∞–∑
_tokenizer: Tokenizer | None = None


def get_tokenizer() -> Tokenizer:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ Qwen3.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω '–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è' (Lazy Loading):
    - –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Å –¥–∏—Å–∫–∞ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ü–ï–†–í–û–ú –≤—ã–∑–æ–≤–µ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    - –í—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –≤—ã–∑–æ–≤—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –∏–∑ –ø–∞–º—è—Ç–∏
    - –≠—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –≤—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å
    
    Returns:
        Tokenizer: –ì–æ—Ç–æ–≤—ã–π –∫ —Ä–∞–±–æ—Ç–µ –æ–±—ä–µ–∫—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        
    Raises:
        FileNotFoundError: –ï—Å–ª–∏ —Ñ–∞–π–ª tokenizer.json –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –æ–∂–∏–¥–∞–µ–º–æ–º—É –ø—É—Ç–∏
        RuntimeError: –ï—Å–ª–∏ —Ñ–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏–ª–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—á–∏—Ç–∞–Ω
    """
    global _tokenizer
    
    # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω ‚Äî —Å—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ (–ø–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ –Ω—É–∂–Ω–∞)
    if _tokenizer is not None:
        return _tokenizer
    
    logger.debug(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑: {_TOKENIZER_FILE}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º
    if not _TOKENIZER_FILE.exists():
        error_msg = (
            f"‚ùå –§–∞–π–ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {_TOKENIZER_FILE}\n\n"
            f"–ß—Ç–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å:\n"
            f"1. –°–∫–∞—á–∞–π—Ç–µ —Ñ–∞–π–ª tokenizer.json –¥–ª—è –º–æ–¥–µ–ª–∏ Qwen3-8B —Å HuggingFace:\n"
            f"   https://huggingface.co/Qwen/Qwen3-8B/tree/main\n"
            f"2. –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É: {_TOKENIZER_DIR}\n"
            f"3. –ü–æ–ª–æ–∂–∏—Ç–µ —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —ç—Ç—É –ø–∞–ø–∫—É –ø–æ–¥ –∏–º–µ–Ω–µ–º: tokenizer.json"
        )
        logger.critical(error_msg)
        raise FileNotFoundError(error_msg)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ JSON-—Ñ–∞–π–ª–∞
        # from_file() ‚Äî –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç –ø—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        _tokenizer = Tokenizer.from_file(str(_TOKENIZER_FILE))
        
        logger.info(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä Qwen3 —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {_TOKENIZER_FILE}")
        logger.debug(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {_tokenizer.get_vocab_size()} —Ç–æ–∫–µ–Ω–æ–≤")
        
    except Exception as e:
        logger.critical(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}", exc_info=True)
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {e}")
    
    return _tokenizer


@lru_cache(maxsize=2048)
def count_tokens_qwen(text: str) -> int:
    """
    –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ –¥–ª—è –º–æ–¥–µ–ª–∏ Qwen3.
    
    –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:
    1. –ü–æ–ª—É—á–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ)
    2. –ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ID —Ç–æ–∫–µ–Ω–æ–≤
    3. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–ª–∏–Ω—É —ç—Ç–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—ç—à (LRU Cache) –Ω–∞ 2048 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫
      ‚Üí –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –≤—ã–∑–æ–≤—ã —Å —Ç–µ–º –∂–µ —Ç–µ–∫—Å—Ç–æ–º –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
    - –ù–µ –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (BOS/EOS), –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç llama.cpp
      ‚Üí –ü–æ–¥—Å—á—ë—Ç —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–º—É –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏
    
    Args:
        text (str): –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤. –î–ª—è –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0
        
    Example:
        >>> count_tokens_qwen("–ü—Ä–∏–≤–µ—Ç, –ö–∞—è!")
        5
        >>> count_tokens_qwen("")  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        0
    """
    # –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç = 0 —Ç–æ–∫–µ–Ω–æ–≤ (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –≤—ã–∑–æ–≤–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞)
    if not text:
        return 0
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ)
        tokenizer = get_tokenizer()
        
        # encode() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç Encoding, —É –∫–æ—Ç–æ—Ä–æ–≥–æ –µ—Å—Ç—å —Å–≤–æ–π—Å—Ç–≤–æ .ids ‚Äî —Å–ø–∏—Å–æ–∫ ID —Ç–æ–∫–µ–Ω–æ–≤
        # len() —Å—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∏—Ö ID = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        token_count = len(tokenizer.encode(text).ids)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ DEBUG, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –ª–æ–≥ –ø—Ä–∏ —á–∞—Å—Ç—ã—Ö –≤—ã–∑–æ–≤–∞—Ö
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 30 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        logger.debug(f"–¢–æ–∫–µ–Ω—ã: '{text[:30]}{'...' if len(text) > 30 else ''}' ‚Üí {token_count}")
        
        return token_count
        
    except Exception as e:
        # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É —Å –ø–æ–ª–Ω—ã–º traceback (exc_info=True) ‚Äî –∫–∞–∫ –≤ main.py
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥—Å—á—ë—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤: {e}", exc_info=True)
        
        # –ù–ï –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–∞–ª—å—à–µ ‚Äî —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –≤–µ—Å—å –ø–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≥—Ä—É–±—É—é –æ—Ü–µ–Ω–∫—É (1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞) –∫–∞–∫ "–∞–≤–∞—Ä–∏–π–Ω—ã–π —Ä–µ–∂–∏–º"
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç —Å–∏—Å—Ç–µ–º–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Ä–∞–±–æ—Ç—É —Å —á—É—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        estimated = max(1, len(text) // 4)
        logger.warning(f"–ò—Å–ø–æ–ª—å–∑—É—é –æ—Ü–µ–Ω–∫—É: ~{estimated} —Ç–æ–∫–µ–Ω–æ–≤ (fallback)")
        return estimated


# === –ë–õ–û–ö –°–ê–ú–û–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ===
# –ï—Å–ª–∏ —Ñ–∞–π–ª –∑–∞–ø—É—Å—Ç–∏–ª–∏ –Ω–∞–ø—Ä—è–º—É—é: python tokens_counter.py
if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –±–∞–∑–æ–≤–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–µ—Å—Ç–∞ (–µ—Å–ª–∏ main.py –µ—â—ë –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è)
    logging.basicConfig(
        level=logging.DEBUG,
        format="[%(asctime)s] %(levelname)-8s | %(name)-15s | %(message)s",
        handlers=[logging.StreamHandler()]
    )
    
    print("\nüß™ –¢–µ—Å—Ç –º–æ–¥—É–ª—è tokens_counter.py")
    print("=" * 50)
    
    test_cases = [
        ("", "–ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞"),
        ("–ü—Ä–∏–≤–µ—Ç", "–ö–æ—Ä–æ—Ç–∫–æ–µ —Å–ª–æ–≤–æ"),
        ("–ü—Ä–∏–≤–µ—Ç, –ö–∞—è! –ö–∞–∫ –¥–µ–ª–∞?", "–ü—Ä–æ—Å—Ç–∞—è —Ñ—Ä–∞–∑–∞"),
        ("The quick brown fox jumps over the lazy dog. " * 3, "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç"),
        ("def hello():\n    print('Hello, World!')", "–ö–æ–¥ Python"),
    ]
    
    try:
        for text, description in test_cases:
            count = count_tokens_qwen(text)
            print(f"‚úì {description:25s} | {count:3d} —Ç–æ–∫–µ–Ω–æ–≤ | '{text[:40]}{'...' if len(text) > 40 else ''}'")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞: {count_tokens_qwen.cache_info()}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –∫—ç—à —Ä–∞–±–æ—Ç–∞–µ—Ç (–ø–æ–≤—Ç–æ—Ä–Ω—ã–π –≤—ã–∑–æ–≤ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–º)
        import time
        start = time.perf_counter()
        _ = count_tokens_qwen("–¢–µ—Å—Ç –∫—ç—à–∞ " * 10)
        cached_time = time.perf_counter() - start
        
        start = time.perf_counter()
        _ = count_tokens_qwen("–¢–µ—Å—Ç –∫—ç—à–∞ " * 10)  # –î–æ–ª–∂–Ω–æ —Å—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑ –∫—ç—à–∞
        cached_time_2 = time.perf_counter() - start
        
        print(f"‚ö° –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤: {cached_time*1000:.2f} –º—Å")
        print(f"‚ö° –ò–∑ –∫—ç—à–∞:      {cached_time_2*1000:.2f} –º—Å")
        
        print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"\n‚ùå –¢–µ—Å—Ç –ø—Ä–æ–≤–∞–ª–µ–Ω: {e}")
        exit(1)